{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ddefcbb-422a-4c2b-b932-60bd7cb220a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup & Imports\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29e3416a-9a28-4efa-9e8f-286da7367ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transforms input images into a latent embedding z.\"\"\"\n",
    "    def __init__(self, latent_dim: int = 128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 28×28 → 28×28\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 28×28 → 14×14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 14×14 → 14×14\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 14×14 → 7×7\n",
    "            nn.Flatten(),     # → (64*7*7)\n",
    "    )\n",
    "        # Project flattened features to latent_dim\n",
    "        self.project = nn.Linear(64 * 7 * 7, latent_dim)  # → z ∈ ℝᵈ\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.feature_extractor(x)\n",
    "        z = self.project(features)\n",
    "        return z # Matches “z ∈ ℝᵈ”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf369eb5-1704-4062-b88a-1aa50882d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture-of-Experts Decoder for EpiNet.\n",
    "\n",
    "    Given latent embedding z_t and recall embedding r_t,\n",
    "    concatenates to h_t ∈ ℝ^{2d}, then uses E parallel experts\n",
    "    and a gating network to produce class logits:\n",
    "      • Gate: g = softmax(G·h_t) ∈ ℝ^E\n",
    "      • Experts: ℓ^{(e)} = expert_e(h_t) ∈ ℝ^K\n",
    "      • logits = Σ_{e=1}^E g_e · ℓ^{(e)}\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_classes: int,\n",
    "        num_experts: int = 4\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_experts = num_experts\n",
    "        # Gating network: 2d → E\n",
    "        self.gate = nn.Linear(latent_dim * 2, num_experts)\n",
    "        # Experts: each maps 2d → hidden_dim → num_classes\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(latent_dim * 2, hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(hidden_dim, num_classes)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, z: torch.Tensor, r: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          z (torch.Tensor): [B, d], latent embedding from Encoder\n",
    "          r (torch.Tensor): [B, d], recall embedding from RecallEngine\n",
    "        Returns:\n",
    "          logits (torch.Tensor): [B, num_classes]\n",
    "        \"\"\"\n",
    "        # Concatenate embeddings: [B, 2d]\n",
    "        h = torch.cat([z, r], dim=1)\n",
    "        # Compute gating weights: [B, E]\n",
    "        gate_logits = self.gate(h)\n",
    "        gate_weights = F.softmax(gate_logits, dim=1)\n",
    "        # Expert outputs: stack into [B, E, K]\n",
    "        expert_outputs = torch.stack(\n",
    "            [expert(h) for expert in self.experts],\n",
    "            dim=1\n",
    "        )\n",
    "        # Weighted sum of experts: [B, K]\n",
    "        gate_weights = gate_weights.unsqueeze(-1)  # [B, E, 1]\n",
    "        logits = (gate_weights * expert_outputs).sum(dim=1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "897efe96-0c17-4b22-aaf6-ee0234091431",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoReplayModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A pure supervised model: encode → decode,\n",
    "    with no episodic memory or replay.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 latent_dim: int = 128,\n",
    "                 hidden_dim: int = 256,\n",
    "                 num_classes: int = 10,\n",
    "                 device: torch.device = None):\n",
    "        super().__init__()\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.encoder = Encoder(latent_dim).to(self.device)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, num_classes).to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor = None):\n",
    "        x = x.to(self.device)\n",
    "        # 1) encode\n",
    "        z = self.encoder(x)                        # [B, latent_dim]\n",
    "        # 2) no recall → zero vector\n",
    "        recall_vec = torch.zeros_like(z)           # [B, latent_dim]\n",
    "        # 3) decode\n",
    "        logits = self.decoder(z, recall_vec)       # [B, num_classes]\n",
    "\n",
    "        if y is None:\n",
    "            return logits\n",
    "\n",
    "        # 4) supervised loss only\n",
    "        y = y.to(self.device)\n",
    "        loss = self.criterion(logits, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15745767-4847-4969-be90-0b763d2bedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_loader(\n",
    "    batch_size:  int,\n",
    "    num_workers: int  = 4,\n",
    "    pin_memory:  bool = True,\n",
    "    test_frac:   float = 0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns four DataLoaders for Split‑MNIST with an 80/20 train‑test split in each task:\n",
    "      - Task1 (digits 0–4): train1, test1\n",
    "      - Task2 (digits 5–9): train2, test2\n",
    "    \"\"\"\n",
    "    # Transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "\n",
    "    # Load full MNIST training set\n",
    "    full = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    targets = full.targets\n",
    "\n",
    "    # Split indices\n",
    "    idx1 = (targets < 5).nonzero(as_tuple=True)[0]\n",
    "    idx2 = (targets >= 5).nonzero(as_tuple=True)[0]\n",
    "    ds1 = Subset(full, idx1)\n",
    "    ds2 = Subset(full, idx2)\n",
    "\n",
    "    # 80/20 split each\n",
    "    def split(ds):\n",
    "        n = len(ds)\n",
    "        n_test = int(n * test_frac)\n",
    "        n_train = n - n_test\n",
    "        return random_split(ds, [n_train, n_test])\n",
    "\n",
    "    train1_ds, test1_ds = split(ds1)\n",
    "    train2_ds, test2_ds = split(ds2)\n",
    "\n",
    "    # DataLoaders\n",
    "    loader_args = dict(batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    train1 = DataLoader(train1_ds, shuffle=True,  **loader_args)\n",
    "    test1  = DataLoader(test1_ds,  shuffle=False, **loader_args)\n",
    "    train2 = DataLoader(train2_ds, shuffle=True,  **loader_args)\n",
    "    test2  = DataLoader(test2_ds,  shuffle=False, **loader_args)\n",
    "\n",
    "    return train1, test1, train2, test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a15aaa0-4039-4cb0-88ed-ff167fdf36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train1, test1, train2, test2 = train_test_split_loader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6513f902-9415-46dd-a15b-c04e698519cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_per_task = 10\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Compute classification accuracy of `model` on `loader`.\n",
    "    Automatically uses whatever device the model’s parameters live on.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)               # returns logits only\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total   += yb.size(0)\n",
    "    return correct / total if total else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd283cfe-1b2b-4f50-8819-542d37026ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "model = NoReplayModel(\n",
    "    latent_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_classes=10,\n",
    "    device=torch.device('cpu')\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop for Task 1\n",
    "for epoch in range(epochs_per_task):\n",
    "    model.train()\n",
    "    for x, y in train1:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate Task 1\n",
    "acc1_after_T1 = evaluate(model, test1)\n",
    "\n",
    "# Training loop for Task 2\n",
    "for epoch in range(epochs_per_task):\n",
    "    model.train()\n",
    "    for x, y in train2:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate Task 1 again\n",
    "acc1_after_T2 = evaluate(model, test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce3f6ffb-80e3-4141-b89a-01b078a2168c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9872528190880863"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1_after_T1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f04f266d-061a-4c1b-9284-a47c23eaf7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1_after_T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a500b136-87e2-4522-9abe-363310b2c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Episodic Memory Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7858c5b8-ba2f-4070-b196-1ecbe2ed6937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MemoryController Module\n",
    "class MemoryController:\n",
    "    \"\"\"\n",
    "    Computes the biologically-inspired salience decay.\n",
    "    salience_m = r * exp(–α · (τ_now – τ_m))\n",
    "\n",
    "    Notation:\n",
    "      • r      … initial salience score (r₀)\n",
    "      • τ_m    … timestamp when memory m was stored\n",
    "      • α      … decay rate (learnable or fixed)\n",
    "      • salience_m … decayed salience at the current time\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: float):\n",
    "        \"\"\"\n",
    "        α: decay rate (higher → faster forgetting)\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def decay(self, r: torch.Tensor, tau_m: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "           Apply decay to a batch of memories.\n",
    "\n",
    "        Args:\n",
    "            r      (torch.Tensor): shape [M], initial salience scores for M memories\n",
    "            tau_m  (torch.Tensor): shape [M], stored timestamps (in seconds) for each memory\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor of shape [M]:\n",
    "              current salience_m = r * exp(–α · (τ_now – τ_m))\n",
    "        \"\"\"\n",
    "        #  Current time in seconds, on same device as inputs\n",
    "        tau_now = torch.tensor(time.time(), device=r.device)\n",
    "        #  Δτ = τ_now – τ_m\n",
    "        delta_tau = tau_now - tau_m\n",
    "        #  Apply decay element-wise\n",
    "        salience_m = r * torch.exp(-self.alpha * delta_tau)\n",
    "        return salience_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd383a0a-a2fc-4e95-82db-d032492c4bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. EpisodicMemory Module\n",
    "class EpisodicMemory:\n",
    "    \"\"\"\n",
    "    Fixed‑capacity buffer storing tuples, m = (z, c, r₀, τₘ, yₘ).\n",
    "    admitting only the most salient memories over time.\n",
    "    Admission & eviction are driven by current salience decay:\n",
    "      • Admit new memory if under capacity,\n",
    "      • Otherwise evict the memory with lowest decayed salience.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            capacity: int,\n",
    "            latent_dim: int,\n",
    "            decay_rate: float,\n",
    "            device: torch.device\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity (int): maximum number of memories\n",
    "            latent_dim (int): dimensionality of the latent space\n",
    "            decay_rate (float): decay rate for salience decay\n",
    "            device (torch.device): device to store the memory\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "\n",
    "        # Memory buffer (initially empty)\n",
    "        self.z_buffer = torch.empty((0, latent_dim), device=device) # z: [N × d] latent embeddings z\n",
    "        self.c_buffer = torch.empty((0, latent_dim), device=device) # c: [N × d] context vectors\n",
    "        self.r0_buffer = torch.empty((0,), device=device)  # r0: [N] initial salience score r₀\n",
    "        self.tau_buffer = torch.empty((0,), device=device) # timestamps τₘ when stored\n",
    "        self.y_buffer = torch.empty((0,), dtype=torch.long, device=device) # labels yₘ\n",
    "\n",
    "        # Reuse your MemoryController for decay\n",
    "        self.mem_ctrl = MemoryController(decay_rate)\n",
    "\n",
    "    def add(\n",
    "            self,\n",
    "            z: torch.Tensor,\n",
    "            c: torch.Tensor,\n",
    "            r0: float,\n",
    "            y: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Try to admit new memory (z,c,r0,τ,y).\n",
    "        If at capacity, evict the lowest‐decayed‐salience memory.\n",
    "        \"\"\"\n",
    "        # Preparing tensors for concatenation\n",
    "        z = z.detach().to(self.device).view(1, -1)\n",
    "        c = c.detach().to(self.device).view(1, -1)\n",
    "        r0 = torch.tensor([r0], device=self.device)\n",
    "        y = y.detach().to(self.device).view(1)\n",
    "        tau = torch.tensor([time.time()], device=self.device)\n",
    "\n",
    "        # If under capacity, just append/admit\n",
    "        if self.z_buffer.shape[0] < self.capacity:\n",
    "            self._append(z, c, r0, tau, y)\n",
    "            return\n",
    "\n",
    "        # Otherwise compute decayed salience of existing memories\n",
    "        s_existing = self.mem_ctrl.decay(self.r0_buffer, self.tau_buffer)\n",
    "\n",
    "        # If this new memory isn't more salience than the least one, skip\n",
    "        if r0 <= s_existing.min():\n",
    "            return\n",
    "\n",
    "        # Else evict the lowest‐salience and replace it\n",
    "        idx = torch.argmin(s_existing).item()\n",
    "        self._replace(idx, z, c, r0, tau, y)\n",
    "\n",
    "    def _append(self, z, c, r0, tau, y):\n",
    "        \"\"\"Add a new memory at the end of each buffer.\"\"\"\n",
    "        self.z_buffer = torch.cat([self.z_buffer, z], dim=0)\n",
    "        self.c_buffer = torch.cat([self.c_buffer, c], dim=0)\n",
    "        self.r0_buffer = torch.cat([self.r0_buffer, r0], dim=0)\n",
    "        self.tau_buffer = torch.cat([self.tau_buffer, tau], dim=0)\n",
    "        self.y_buffer = torch.cat([self.y_buffer, y], dim=0)\n",
    "\n",
    "\n",
    "    def _replace(self, idx, z, c, r0, tau, y):\n",
    "        \"\"\"Overwrite the memory at index `idx` with the new one.\"\"\"\n",
    "        self.z_buffer[idx] = z\n",
    "        self.c_buffer[idx] = c\n",
    "        self.r0_buffer[idx] = r0\n",
    "        self.tau_buffer[idx] = tau\n",
    "        self.y_buffer[idx] = y\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Reset all buffers to empty.\"\"\"\n",
    "        self.__init__(self.capacity, self.z_buffer.size(1), self.mem_ctrl.alpha, self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29a54d51-ea38-4b8f-a082-23eafe3da7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MemoryController Module\n",
    "class MemoryController:\n",
    "    \"\"\"\n",
    "    Computes the biologically-inspired salience decay.\n",
    "    salience_m = r * exp(–α · (τ_now – τ_m))\n",
    "\n",
    "    Notation:\n",
    "      • r      … initial salience score (r₀)\n",
    "      • τ_m    … timestamp when memory m was stored\n",
    "      • α      … decay rate (learnable or fixed)\n",
    "      • salience_m … decayed salience at the current time\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: float):\n",
    "        \"\"\"\n",
    "        α: decay rate (higher → faster forgetting)\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def decay(self, r: torch.Tensor, tau_m: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "           Apply decay to a batch of memories.\n",
    "\n",
    "        Args:\n",
    "            r      (torch.Tensor): shape [M], initial salience scores for M memories\n",
    "            tau_m  (torch.Tensor): shape [M], stored timestamps (in seconds) for each memory\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor of shape [M]:\n",
    "              current salience_m = r * exp(–α · (τ_now – τ_m))\n",
    "        \"\"\"\n",
    "        #  Current time in seconds, on same device as inputs\n",
    "        tau_now = torch.tensor(time.time(), device=r.device)\n",
    "        #  Δτ = τ_now – τ_m\n",
    "        delta_tau = tau_now - tau_m\n",
    "        #  Apply decay element-wise\n",
    "        salience_m = r * torch.exp(-self.alpha * delta_tau)\n",
    "        return salience_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ff1d237-b131-4d47-a33f-7ad496c65655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RecallEngine Module\n",
    "# - **top_k**: number of highest‑scoring memories to retrieve  \n",
    "# - **RecallScoreₘ** = cos(z_t, cₘ) · salienceₘ  \n",
    "# - **salienceₘ** = r₀ₘ · exp(–α·(τ_now – τₘ))  \n",
    "# - **Recall embedding**: where the sum is over the Top‑K memories.\n",
    "\n",
    "class RecallEngine:\n",
    "    \"\"\"\n",
    "    Handles salience decay, scoring, and top-k retrieval from EpisodicMemory.\n",
    "    Retrieves salient memories based on cosine similarity and decayed salience.\n",
    "\n",
    "    Given a query embedding z_t and stored memories (z_m, c_m, r0_m, τ_m),\n",
    "    computes for each memory:\n",
    "      RecallScore_m = cos(z_t, c_m) * salience_m\n",
    "    where salience_m = r0_m * exp(-α * (τ_now - τ_m)).\n",
    "    Selects Top‑K memories by RecallScore, then computes recall embedding:\n",
    "      r_t = (1 / Σ_i r_i) * Σ_i r_i * z_i,\n",
    "    where r_i is the decayed salience of the selected memories.\n",
    "    \"\"\"\n",
    "    def __init__(self, top_k: int):\n",
    "        \"\"\"\n",
    "        top_k: number of highest‑scoring memories to retrieve.\n",
    "        \"\"\"\n",
    "        self.top_k = int(top_k)\n",
    "\n",
    "    def recall(self, z_query: torch.Tensor, memory: EpisodicMemory) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform memory recall for a batch of query embeddings.\n",
    "\n",
    "        Args:\n",
    "            z_query (torch.Tensor): shape [B, d], query latent embeddings z_t.\n",
    "            memory (EpisodicMemory): contains buffers:\n",
    "              - c_buffer: [N, d] context embeddings c_m\n",
    "              - z_buffer: [N, d] latent embeddings z_m\n",
    "              - r0_buffer: [N]   initial salience r0_m\n",
    "              - tau_buffer: [N]  timestamps τ_m\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: shape [B, d], recall embeddings r_t.\n",
    "        \"\"\"\n",
    "        # Guard for empty memory\n",
    "        if memory.z_buffer.size(0) == 0:\n",
    "            return torch.zeros_like(z_query)\n",
    "\n",
    "        # Compute decayed salience for all stored memories: [N]\n",
    "        salience = memory.mem_ctrl.decay(memory.r0_buffer, memory.tau_buffer)\n",
    "\n",
    "        # Compute cosine similarity: [B, N]\n",
    "        cos_sim = F.cosine_similarity(\n",
    "            z_query.unsqueeze(1),         # [B, 1, d]\n",
    "            memory.c_buffer.unsqueeze(0),   # [1, N, d]\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        # Compute recall scores for selection: [B, N]\n",
    "        recall_scores = cos_sim * salience.unsqueeze(0)\n",
    "\n",
    "        # Select Top-K indices by recall score: [B, K]\n",
    "        _, top_idx = torch.topk(recall_scores, self.top_k, dim=1)\n",
    "\n",
    "        # Gather salience and latent embeddings of selected memories\n",
    "        salience_topk = salience[top_idx]          # [B, K]\n",
    "        z_topk = memory.z_buffer[top_idx]   # [B, K, d]\n",
    "\n",
    "        # Normalize by sum of salience: weights = r_i / Σ_j r_j\n",
    "        weights = salience_topk / (salience_topk.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "        # Weighted sum to form recall embedding: [B, d]\n",
    "        recall_emb = (weights.unsqueeze(-1) * z_topk).sum(dim=1)\n",
    "        return recall_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d15ec6f-f8b2-487a-82ed-2111bc099240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpiNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    EpiNetModel integrates:\n",
    "      1) Encoder → latent embedding z_t\n",
    "      2) RecallEngine/EpisodicMemory → recall embedding r_t\n",
    "      3) Decoder → class logits\n",
    "    Computes joint loss:\n",
    "      L_total = L_task + λ·L_replay\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_classes: int,\n",
    "        capacity: int,\n",
    "        decay_rate: float,\n",
    "        top_k: int,\n",
    "        lambda_coef: float,\n",
    "        device: torch.device = None\n",
    "    ):\n",
    "        super(EpiNetModel, self).__init__()\n",
    "        self.device      = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Core modules\n",
    "        self.encoder       = Encoder(latent_dim)\n",
    "        self.decoder       = Decoder(latent_dim, hidden_dim, num_classes)\n",
    "        self.memory        = EpisodicMemory(capacity, latent_dim, decay_rate, self.device)\n",
    "        self.recall_engine = RecallEngine(top_k)\n",
    "\n",
    "        # Loss and hyperparameters\n",
    "        self.criterion   = nn.CrossEntropyLoss()\n",
    "        self.lambda_coef = lambda_coef\n",
    "\n",
    "        # Move to device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor = None) -> torch.Tensor:\n",
    "        x = x.to(self.device)\n",
    "        # Encode\n",
    "        z = self.encoder(x)\n",
    "        # Recall\n",
    "        r = self.recall_engine.recall(z, self.memory)\n",
    "        # Predict\n",
    "        logits = self.decoder(z, r)\n",
    "        if y is None:\n",
    "            return logits\n",
    "\n",
    "        # Task loss\n",
    "        y = y.to(self.device)\n",
    "        loss_task = self.criterion(logits, y)\n",
    "\n",
    "        # --- Replay loss per Core Math with capacity guard ---\n",
    "        # Decayed salience: [N]\n",
    "        salience_mem = self.memory.mem_ctrl.decay(\n",
    "            self.memory.r0_buffer,\n",
    "            self.memory.tau_buffer\n",
    "        )\n",
    "        # Cosine similarity [B, N]\n",
    "        cos_sim_mem = F.cosine_similarity(\n",
    "            z.unsqueeze(1),                # [B,1,d]\n",
    "            self.memory.c_buffer.unsqueeze(0),  # [1,N,d]\n",
    "            dim=-1\n",
    "        )\n",
    "        # Recall scores [B, N]\n",
    "        recall_scores = cos_sim_mem * salience_mem.unsqueeze(0)\n",
    "\n",
    "        # Only compute replay if we have any memories\n",
    "        N = self.memory.z_buffer.size(0)\n",
    "        if N > 0:\n",
    "            # clamp k by current memory size\n",
    "            k = min(self.recall_engine.top_k, N)\n",
    "            _, top_idx = torch.topk(\n",
    "                recall_scores,\n",
    "                k,\n",
    "                dim=1\n",
    "            )\n",
    "            # Gather for replay\n",
    "            salience_topk = salience_mem[top_idx]        # [B,K]\n",
    "            z_topk        = self.memory.z_buffer[top_idx] # [B,K,d]\n",
    "            y_topk        = self.memory.y_buffer[top_idx] # [B,K]\n",
    "            # Flatten for batch decode\n",
    "            B, K, d = z_topk.shape\n",
    "            z_flat = z_topk.view(B*K, d)\n",
    "            r_flat = torch.zeros_like(z_flat)\n",
    "            y_flat = y_topk.view(B*K)\n",
    "            # Replay logits and per-item losses\n",
    "            logits_mem = self.decoder(z_flat, r_flat)\n",
    "            losses_mem = F.cross_entropy(logits_mem, y_flat, reduction='none').view(B, K)\n",
    "            # Weighted sum for replay loss\n",
    "            loss_replay = (salience_topk * losses_mem).sum()\n",
    "            # Total loss with replay\n",
    "            loss = loss_task + self.lambda_coef * loss_replay\n",
    "        else:\n",
    "            # No memories yet, skip replay\n",
    "            loss = loss_task\n",
    "\n",
    "        # Update episodic memory\n",
    "        with torch.no_grad():\n",
    "            initial_r0 = 1.0\n",
    "            for zi, yi in zip(z, y):\n",
    "                # Use z as both embedding & context\n",
    "                self.memory.add(zi, zi, initial_r0, yi)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ca581b7-4271-4642-bb7f-20637f48b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "hidden_dim = 32\n",
    "num_classes = 10\n",
    "capacity = 20\n",
    "decay_rate = 0.05\n",
    "top_k = 5\n",
    "lambda_coef = 0.5\n",
    "device = torch.device('cpu')\n",
    "model = EpiNetModel(\n",
    "    latent_dim, hidden_dim, num_classes,\n",
    "    capacity, decay_rate, top_k, lambda_coef,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "729d40a3-d324-41e9-828e-63880ebcb5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            logits = model(xb)                  # uses forward(x) → logits only\n",
    "            preds  = logits.argmax(dim=1)\n",
    "            correct += (preds == yb.to(logits.device)).sum().item()\n",
    "            total   += yb.size(0)\n",
    "    return correct/total if total else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97375452-19bc-4018-bf17-02aff5f21eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_task(\n",
    "    task_id: int,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader:  DataLoader,\n",
    "    model:        nn.Module,\n",
    "    optimizer:    torch.optim.Optimizer,\n",
    "    scaler:       GradScaler,\n",
    "    epochs:       int\n",
    "):\n",
    "    print(f\"=== Training Task {task_id} ===\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(model.device), yb.to(model.device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                loss = model(xb, yb)        # memory is updated inside forward\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        acc = evaluate(model, test_loader)\n",
    "        print(f\"[Task {task_id} · epoch {epoch}] loss={avg_loss:.4f}, test_acc={acc:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0a8f413-8b0b-4a5a-bc65-7606c3d4eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without clearing memory after task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d21e7f6-8546-40e6-99b7-d0155ac69b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Task 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108350/3365339607.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler()\n",
      "/home/collin/anaconda3/envs/py3_env/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_108350/1676787063.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/collin/anaconda3/envs/py3_env/lib/python3.10/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 1 · epoch 1] loss=524.9949, test_acc=0.9745\n",
      "[Task 1 · epoch 2] loss=243.3017, test_acc=0.9850\n",
      "[Task 1 · epoch 3] loss=100.5207, test_acc=0.9871\n",
      "[Task 1 · epoch 4] loss=135.0226, test_acc=0.9874\n",
      "[Task 1 · epoch 5] loss=50.1477, test_acc=0.9856\n",
      "[Task 1 · epoch 6] loss=18.5658, test_acc=0.9912\n",
      "[Task 1 · epoch 7] loss=4.8374, test_acc=0.9905\n",
      "[Task 1 · epoch 8] loss=3.0064, test_acc=0.9873\n",
      "[Task 1 · epoch 9] loss=1.1887, test_acc=0.9833\n",
      "[Task 1 · epoch 10] loss=0.0381, test_acc=0.9877\n",
      "\n",
      "Task1 end-of-training acc: 0.9877430952770061\n",
      "=== Training Task 2 ===\n",
      "[Task 2 · epoch 1] loss=2.6182, test_acc=0.0000\n",
      "[Task 2 · epoch 2] loss=2.3647, test_acc=0.0000\n",
      "[Task 2 · epoch 3] loss=2.3410, test_acc=0.0000\n",
      "[Task 2 · epoch 4] loss=2.3160, test_acc=0.0000\n",
      "[Task 2 · epoch 5] loss=2.2898, test_acc=0.0000\n",
      "[Task 2 · epoch 6] loss=2.2626, test_acc=0.2060\n",
      "[Task 2 · epoch 7] loss=2.2345, test_acc=0.2060\n",
      "[Task 2 · epoch 8] loss=440.0653, test_acc=0.2060\n",
      "[Task 2 · epoch 9] loss=410.3359, test_acc=0.2060\n",
      "[Task 2 · epoch 10] loss=344.5708, test_acc=0.2060\n",
      "\n",
      "Final acc Task1: 0.0000, Task2: 0.2060\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare data\n",
    "train1, test1, train2, test2 = train_test_split_loader(batch_size=128)\n",
    "\n",
    "# 2. Instantiate model, optimizer, scaler\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scaler    = GradScaler()\n",
    "epochs    = 10\n",
    "\n",
    "# 3. Task 1\n",
    "train_one_task(1, train1, test1, model, optimizer, scaler, epochs)\n",
    "\n",
    "# 4. inspect Task1 retention\n",
    "acc1_postT1 = evaluate(model, test1)\n",
    "print(\"Task1 end-of-training acc:\", acc1_postT1)\n",
    "\n",
    "# 5. Task 2 (memory remains, so replay is possible)\n",
    "train_one_task(2, train2, test2, model, optimizer, scaler, epochs)\n",
    "\n",
    "# 6. Final joint eval\n",
    "acc1_final = evaluate(model, test1)\n",
    "acc2_final = evaluate(model, test2)\n",
    "print(f\"Final acc Task1: {acc1_final:.4f}, Task2: {acc2_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "379e7134-0e8e-4547-aeaf-5b610d5823db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9877430952770061"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1_postT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d5ce540-b17b-4f9b-9294-fe7947944b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6bbf929-e273-4872-8120-27d9bd512eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20595238095238094"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc2_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bd1e9c3-2145-4e6f-b079-7b64ddd42938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Task 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108350/2606081374.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler()\n",
      "/tmp/ipykernel_108350/1676787063.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 1 · epoch 1] loss=147.3190, test_acc=0.9773\n",
      "[Task 1 · epoch 2] loss=16.4186, test_acc=0.9922\n",
      "[Task 1 · epoch 3] loss=3.2950, test_acc=0.9922\n",
      "[Task 1 · epoch 4] loss=0.6319, test_acc=0.9912\n",
      "[Task 1 · epoch 5] loss=0.2859, test_acc=0.9900\n",
      "[Task 1 · epoch 6] loss=0.1678, test_acc=0.9956\n",
      "[Task 1 · epoch 7] loss=0.0990, test_acc=0.9935\n",
      "[Task 1 · epoch 8] loss=0.0697, test_acc=0.9946\n",
      "[Task 1 · epoch 9] loss=0.0596, test_acc=0.9946\n",
      "[Task 1 · epoch 10] loss=0.1681, test_acc=0.9941\n",
      "\n",
      "Task1 end-of-training acc: 0.9941166857329629\n",
      "=== Training Task 2 ===\n",
      "[Task 2 · epoch 1] loss=139.4774, test_acc=0.9517\n",
      "[Task 2 · epoch 2] loss=0.4626, test_acc=0.9702\n",
      "[Task 2 · epoch 3] loss=0.4414, test_acc=0.9794\n",
      "[Task 2 · epoch 4] loss=0.2438, test_acc=0.9838\n",
      "[Task 2 · epoch 5] loss=0.1068, test_acc=0.9845\n",
      "[Task 2 · epoch 6] loss=0.0812, test_acc=0.9864\n",
      "[Task 2 · epoch 7] loss=0.0677, test_acc=0.9872\n",
      "[Task 2 · epoch 8] loss=0.0579, test_acc=0.9891\n",
      "[Task 2 · epoch 9] loss=0.5099, test_acc=0.9757\n",
      "[Task 2 · epoch 10] loss=0.0950, test_acc=0.9886\n",
      "\n",
      "Final acc Task1: 0.0000, Task2: 0.9886\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare data\n",
    "train1, test1, train2, test2 = train_test_split_loader(batch_size=128)\n",
    "\n",
    "# 2. Instantiate model, optimizer, scaler\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scaler    = GradScaler()\n",
    "epochs    = 10\n",
    "\n",
    "# 3. Task 1\n",
    "train_one_task(1, train1, test1, model, optimizer, scaler, epochs)\n",
    "\n",
    "# 4. inspect Task1 retention\n",
    "acc1_postT1 = evaluate(model, test1)\n",
    "print(\"Task1 end-of-training acc:\", acc1_postT1)\n",
    "\n",
    "# 5. Clear memory before Task 2 (fresh slate)\n",
    "model.memory.clear()\n",
    "train_one_task(2, train2, test2, model, optimizer, scaler, epochs)\n",
    "\n",
    "# 6. Final joint eval\n",
    "acc1_final = evaluate(model, test1)\n",
    "acc2_final = evaluate(model, test2)\n",
    "print(f\"Final acc Task1: {acc1_final:.4f}, Task2: {acc2_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1dae2cf9-ef37-48f6-acd7-278e0876f999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9941166857329629"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1_postT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3e8e499-2993-45d3-a0b8-4d4630c642bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b487f-4cca-4a30-86d5-999dbc8ef1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57be3696-a2ff-4be4-bcb6-782761aa1d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "hidden_dim = 32\n",
    "num_classes = 10\n",
    "capacity = 5000\n",
    "decay_rate = 0.05\n",
    "top_k = 5\n",
    "lambda_coef = 0.5\n",
    "device = torch.device('cpu')\n",
    "model = EpiNetModel(\n",
    "    latent_dim, hidden_dim, num_classes,\n",
    "    capacity, decay_rate, top_k, lambda_coef,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "080905f0-4744-47de-92dc-8ed04a16f995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Task 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108350/3227440734.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler()\n",
      "/tmp/ipykernel_108350/1676787063.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 1 · epoch 1] loss=170.8623, test_acc=0.9734\n",
      "[Task 1 · epoch 2] loss=34.0531, test_acc=0.9863\n",
      "[Task 1 · epoch 3] loss=25.6282, test_acc=0.9884\n",
      "[Task 1 · epoch 4] loss=17.4391, test_acc=0.9917\n",
      "[Task 1 · epoch 5] loss=6.2336, test_acc=0.9936\n",
      "[Task 1 · epoch 6] loss=5.5208, test_acc=0.9917\n",
      "[Task 1 · epoch 7] loss=5.1656, test_acc=0.9956\n",
      "[Task 1 · epoch 8] loss=5.3923, test_acc=0.9943\n",
      "[Task 1 · epoch 9] loss=2.6825, test_acc=0.9943\n",
      "[Task 1 · epoch 10] loss=1.0483, test_acc=0.9946\n",
      "\n",
      "Task1 end-of-training acc: 0.9946069619218827\n",
      "=== Training Task 2 ===\n",
      "[Task 2 · epoch 1] loss=1.5114, test_acc=0.7816\n",
      "[Task 2 · epoch 2] loss=0.4180, test_acc=0.9262\n",
      "[Task 2 · epoch 3] loss=37.8773, test_acc=0.9471\n",
      "[Task 2 · epoch 4] loss=60.0141, test_acc=0.9668\n",
      "[Task 2 · epoch 5] loss=42.7368, test_acc=0.9764\n",
      "[Task 2 · epoch 6] loss=29.0783, test_acc=0.9759\n",
      "[Task 2 · epoch 7] loss=22.6780, test_acc=0.9794\n",
      "[Task 2 · epoch 8] loss=11.2401, test_acc=0.9830\n",
      "[Task 2 · epoch 9] loss=10.1574, test_acc=0.9835\n",
      "[Task 2 · epoch 10] loss=9.1835, test_acc=0.9833\n",
      "\n",
      "Final acc Task1: 0.0000, Task2: 0.9833\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare data\n",
    "train1, test1, train2, test2 = train_test_split_loader(batch_size=128)\n",
    "\n",
    "# 2. Instantiate model, optimizer, scaler\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scaler    = GradScaler()\n",
    "epochs    = 10\n",
    "\n",
    "# 3. Task 1\n",
    "train_one_task(1, train1, test1, model, optimizer, scaler, epochs)\n",
    "\n",
    "# 4. inspect Task1 retention\n",
    "acc1_postT1 = evaluate(model, test1)\n",
    "print(\"Task1 end-of-training acc:\", acc1_postT1)\n",
    "\n",
    "# 5. Task 2 (memory remains, so replay is possible)\n",
    "train_one_task(2, train2, test2, model, optimizer, scaler, epochs)\n",
    "\n",
    "# 6. Final joint eval\n",
    "acc1_final = evaluate(model, test1)\n",
    "acc2_final = evaluate(model, test2)\n",
    "print(f\"Final acc Task1: {acc1_final:.4f}, Task2: {acc2_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b7234-3976-4e03-b8b7-6b541fa4cda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cda83edc-ea84-452a-8d16-fbd9153716d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "hidden_dim = 32\n",
    "num_classes = 10\n",
    "capacity = 50000\n",
    "decay_rate = 0.05\n",
    "top_k = 5\n",
    "lambda_coef = 0.5\n",
    "device = torch.device('cpu')\n",
    "model = EpiNetModel(\n",
    "    latent_dim, hidden_dim, num_classes,\n",
    "    capacity, decay_rate, top_k, lambda_coef,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae4275e8-f873-49ff-b04c-4a2740c95a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Task 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108350/3227440734.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler()\n",
      "/tmp/ipykernel_108350/1676787063.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 1 · epoch 1] loss=155.1899, test_acc=0.9632\n",
      "[Task 1 · epoch 2] loss=28.1807, test_acc=0.9788\n",
      "[Task 1 · epoch 3] loss=13.0096, test_acc=0.9856\n",
      "[Task 1 · epoch 4] loss=7.7389, test_acc=0.9886\n",
      "[Task 1 · epoch 5] loss=5.7921, test_acc=0.9920\n",
      "[Task 1 · epoch 6] loss=4.8277, test_acc=0.9940\n",
      "[Task 1 · epoch 7] loss=3.1445, test_acc=0.9949\n",
      "[Task 1 · epoch 8] loss=3.1893, test_acc=0.9941\n",
      "[Task 1 · epoch 9] loss=3.1580, test_acc=0.9956\n",
      "[Task 1 · epoch 10] loss=2.2608, test_acc=0.9951\n",
      "\n",
      "Task1 end-of-training acc: 0.9950972381108024\n",
      "=== Training Task 2 ===\n",
      "[Task 2 · epoch 1] loss=171.7361, test_acc=0.9789\n",
      "[Task 2 · epoch 2] loss=11.2597, test_acc=0.9849\n",
      "[Task 2 · epoch 3] loss=7.3447, test_acc=0.9840\n",
      "[Task 2 · epoch 4] loss=5.2615, test_acc=0.9869\n",
      "[Task 2 · epoch 5] loss=5.1503, test_acc=0.9878\n",
      "[Task 2 · epoch 6] loss=3.4098, test_acc=0.9889\n",
      "[Task 2 · epoch 7] loss=3.4443, test_acc=0.9900\n",
      "[Task 2 · epoch 8] loss=2.2764, test_acc=0.9881\n",
      "[Task 2 · epoch 9] loss=2.2736, test_acc=0.9906\n",
      "[Task 2 · epoch 10] loss=1.8487, test_acc=0.9886\n",
      "\n",
      "Final acc Task1: 0.0000, Task2: 0.9886\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare data\n",
    "train1, test1, train2, test2 = train_test_split_loader(batch_size=128)\n",
    "\n",
    "# 2. Instantiate model, optimizer, scaler\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scaler    = GradScaler()\n",
    "epochs    = 10\n",
    "\n",
    "# 3. Task 1\n",
    "train_one_task(1, train1, test1, model, optimizer, scaler, epochs)\n",
    "\n",
    "# 4. inspect Task1 retention\n",
    "acc1_postT1 = evaluate(model, test1)\n",
    "print(\"Task1 end-of-training acc:\", acc1_postT1)\n",
    "\n",
    "# 5. Task 2 (memory remains, so replay is possible)\n",
    "train_one_task(2, train2, test2, model, optimizer, scaler, epochs)\n",
    "\n",
    "# 6. Final joint eval\n",
    "acc1_final = evaluate(model, test1)\n",
    "acc2_final = evaluate(model, test2)\n",
    "print(f\"Final acc Task1: {acc1_final:.4f}, Task2: {acc2_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44734000-19ba-44ab-b111-82a7fd5bf660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
